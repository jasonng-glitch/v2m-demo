<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Paper Demo Page</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      max-width: 900px;
      margin: 0 auto;
      padding: 20px;
      line-height: 1.6;
    }
    h1 {
      text-align: center;
      font-size: 2.2em;
    }
    h2 {
      margin-top: 2em;
      border-bottom: 2px solid #ddd;
      padding-bottom: 0.3em;
    }
    .authors {
      text-align: center;
      font-style: italic;
      margin-bottom: 2em;
    }
    .abstract {
      background: #f9f9f9;
      padding: 15px;
      border-radius: 6px;
    }
    .image-section, .video-section {
      text-align: center;
      margin: 2em 0;
    }
    .video-section video {
      width: 100%;
      max-width: 600px;
      margin: 10px;
      border-radius: 8px;
      box-shadow: 0px 2px 6px rgba(0,0,0,0.2);
    }
    img {
      max-width: 100%;
      height: auto;
      border-radius: 8px;
      box-shadow: 0px 2px 6px rgba(0,0,0,0.2);
    }
  </style>
</head>
<body>

  <h1>VISUAL KEYSTOSYMPHONIES:LATENTDIFFUSIONFORMULTI-SCENE VIDEO-TO-MUSIC GENERATION</h1>
  <div class="authors">
    Chiu Fai Ng⋆†, Karpser So⋆, Jing Yang⋆, Patricio Ovalle⋆, Simon Lui⋆, Fan Fan⋆, Yuhan Dong
    <div>⋆ Central Media Technology Institute, Huawei</div>
    <div>† Shenzhen International Graduate School, Tsinghua University</div>
  </div>

  <h2>Overview</h2>
  <div class="overview">
    <p>
        The generation of coherent and emotionally resonant music for multi-scene videos remains a central challenge in the video-to-music (V2M) domain. We introduce a latent diffusion framework that addresses this by synthesizing globally coherent, high-fidelity background scores. Our approach moves beyond trivial audio visual synchronization, establishing culturally-grounded semantic correspondences between the visual narrative and resulting acoustic properties. To facilitate this, we introduced a scalable data pipeline for synthesizing large-scale, audiovisuals-aligned pre-training data. The generative process is conditioned on a combination of visual keyframes and extracted affective cues, and the model is then fine-tuned using Direct Preference Optimization (DPO) to directly align its outputs with human aesthetic preferences. Extensive evaluations on established benchmarks demonstrate that our method achieves comparable performance with state-of-the-art models, yielding superior semantic and temporal alignment. 
    </p>
  </div>

  <h2>Model Architecture</h2>
  <div class="image-section">
    <img src="model.png" alt="Model Diagram">
  </div>

  <h2>Video Demos</h2>
  <div class="video-section">
    <video controls>
      <source src="demo1.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>

    <video controls>
      <source src="demo2.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>

    <video controls>
      <source src="demo3.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>
  </div>

</body>
</html>
